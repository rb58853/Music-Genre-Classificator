{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_percentaje = 0.8\n",
    "validation_percentaje = 0.1\n",
    "test_percentaje = 0.1\n",
    "\n",
    "genres = ['blues', 'classical', 'country', 'disco',\n",
    "          'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select data for each model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def index_genre(genre, genres):\n",
    "    for (g,index) in zip(genres, range(len(genres))):\n",
    "        if(g == genre):\n",
    "            return index\n",
    "    return -1\n",
    "\n",
    "def get_data(data_path, genres, decoder, training_percentaje=0.6, validation_percentaje=0.2, test_percentaje=0.2):\n",
    "    \"\"\" \n",
    "    data_path: se le pasa la direccion de la carpeta donde se encuentra la base de datos.\n",
    "    genres: se le pasa una lista con los nombres da cada carpeta que contiene un genero dado.    \n",
    "    decoder: funcion para decodificar el dato que se le pasa, por ejemplo en caso de imagenes habria hacer imread\n",
    "    \"\"\"\n",
    "    \n",
    "    data_training = {'in': [], 'out': []}\n",
    "    data_validation = {'in': [], 'out': []}\n",
    "    data_test = {'in': [], 'out': []}\n",
    "\n",
    "    for genre in genres:\n",
    "        files = os.listdir(data_path + genre)\n",
    "        count = len(files)\n",
    "\n",
    "        for (filename, index) in zip(files, range(len(files))):\n",
    "            filepath = data_path + genre + '/' + filename\n",
    "\n",
    "            if (index < training_percentaje * count):\n",
    "                data_training['in'].append(decoder(filepath))\n",
    "                data_training['out'].append(index_genre(genre, genres))\n",
    "                continue\n",
    "\n",
    "            if (index < (training_percentaje + validation_percentaje) * count):\n",
    "                data_validation['in'].append(decoder(filepath))\n",
    "                data_validation['out'].append(index_genre(genre, genres))\n",
    "            else:\n",
    "                data_test['in'].append(decoder(filepath))\n",
    "                data_test['out'].append(index_genre(genre, genres))\n",
    "\n",
    "    data_training = {'in': np.array(data_training['in']),'out': np.array(data_training['out'])}            \n",
    "    data_validation = {'in': np.array(data_validation['in']),'out': np.array(data_validation['out'])}            \n",
    "    data_test = {'in': np.array(data_test['in']),'out': np.array(data_test['out'])}            \n",
    "\n",
    "    return {\n",
    "        'data_training': data_training,\n",
    "        'data_validation': data_validation,\n",
    "        'data_testing': data_test\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data for MFCC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "data_path = 'url'\n",
    "\n",
    "def read_image(filepath):\n",
    "    global size_images\n",
    "    image = cv2.imread(filepath)\n",
    "    return cv2.resize(image, size_images)\n",
    "\n",
    "def get_data_mfcc(training_percentaje=0.8, validation_percentaje=0.1, test_percentaje=0.1):\n",
    "    return get_data(data_path, genres, read_image,training_percentaje,validation_percentaje,test_percentaje)\n",
    "\n",
    "mfcc_data = get_data_mfcc(train_percentaje, validation_percentaje, test_percentaje)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data for wavelet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from collections import Counter\n",
    "import scipy\n",
    "import dtcwt\n",
    "\n",
    "data_path = 'url'\n",
    "\n",
    "trans = dtcwt.Transform1d(biort='antonini', qshift='qshift_d')\n",
    "\n",
    "def calculate_entropy(list_values):\n",
    "\tcounter_values = Counter(list_values).most_common()\n",
    "\tprobabilities = [elem[1]/len(list_values) for elem in counter_values]\n",
    "\tentropy=scipy.stats.entropy(probabilities)\n",
    "\treturn entropy\n",
    "\n",
    "def calculate_statistics(list_values):\n",
    "\tn5 = np.nanpercentile(list_values, 5)\n",
    "\tn25 = np.nanpercentile(list_values, 25)\n",
    "\tn75 = np.nanpercentile(list_values, 75)\n",
    "\tn95 = np.nanpercentile(list_values, 95)\n",
    "\tmedian = np.nanpercentile(list_values, 50)\n",
    "\tmean = np.nanmean(list_values)\n",
    "\tstd = np.nanstd(list_values)\n",
    "\tvar = np.nanvar(list_values)\n",
    "\trms = np.nanmean(np.sqrt(list_values**2))\n",
    "\treturn [n5, n25, n75, n95, median, mean, std, var, rms]\n",
    "\n",
    "def calculate_crossings(list_values):\n",
    "\tzero_crossing_indices = np.nonzero(np.diff(np.array(list_values) > 0))[0]\n",
    "\tno_zero_crossings = len(zero_crossing_indices)\n",
    "\tmean_crossing_indices = np.nonzero(np.diff(np.array(list_values) > np.nanmean(list_values)))[0]\n",
    "\tno_mean_crossings = len(mean_crossing_indices)\n",
    "\treturn [no_zero_crossings, no_mean_crossings]\n",
    "\n",
    "def get_features(list_values):\n",
    "\tentropy = calculate_entropy(list_values)\n",
    "\tcrossings = calculate_crossings(list_values)\n",
    "\tstatistics = calculate_statistics(list_values)\n",
    "\treturn [entropy] + crossings + statistics\n",
    "\n",
    "def extract_dtcwt(file_path:str):    \n",
    "    d, fs = librosa.load(file_path)\n",
    "    forw = trans.forward(d, nlevels=17)\n",
    "    features = []\n",
    "    for coeff in forw.highpasses:\n",
    "        temp = (np.abs(coeff.squeeze()))\n",
    "        features += get_features(temp)\n",
    "        \n",
    "    features += get_features(forw.lowpass.squeeze())    \n",
    "    return features\n",
    "\n",
    "def get_data_wavelet(training_percentaje=0.8, validation_percentaje=0.1, test_percentaje=0.1):\n",
    "    return get_data(data_path, genres, extract_dtcwt, training_percentaje,validation_percentaje,test_percentaje)\n",
    "\n",
    "wavalet_data = get_data_mfcc(train_percentaje, validation_percentaje, test_percentaje)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill all datas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = { 'mfcc': mfcc_data,\n",
    "         'wavalet': wavalet_data\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import model.h5 for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    train_data = []\n",
    "    for key,model in models.items():\n",
    "        train_data.append(model.predict(data[key]['train']))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
