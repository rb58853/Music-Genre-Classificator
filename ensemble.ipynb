{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_percentaje = 0.8\n",
    "validation_percentaje = 0.1\n",
    "test_percentaje = 0.1\n",
    "\n",
    "genres = ['blues', 'classical', 'country', 'disco',\n",
    "          'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select data for each model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def index_genre(genre, genres):\n",
    "    for (g,index) in zip(genres, range(len(genres))):\n",
    "        if(g == genre):\n",
    "            return index\n",
    "    return -1\n",
    "\n",
    "def get_data(data_path, genres, decoder, training_percentaje=0.6, validation_percentaje=0.2, test_percentaje=0.2):\n",
    "    \"\"\" \n",
    "    data_path: se le pasa la direccion de la carpeta donde se encuentra la base de datos.\n",
    "    genres: se le pasa una lista con los nombres da cada carpeta que contiene un genero dado.    \n",
    "    decoder: funcion para decodificar el dato que se le pasa, por ejemplo en caso de imagenes habria hacer imread\n",
    "    \"\"\"\n",
    "    \n",
    "    data_training = {'in': [], 'out': []}\n",
    "    data_validation = {'in': [], 'out': []}\n",
    "    data_test = {'in': [], 'out': []}\n",
    "\n",
    "    for genre in genres:\n",
    "        files = os.listdir(data_path + genre)\n",
    "        count = len(files)\n",
    "\n",
    "        for (filename, index) in zip(files, range(len(files))):\n",
    "            filepath = data_path + genre + '/' + filename\n",
    "\n",
    "            if (index < training_percentaje * count):\n",
    "                data_training['in'].append(decoder(filepath))\n",
    "                data_training['out'].append(index_genre(genre, genres))\n",
    "                continue\n",
    "\n",
    "            if (index < (training_percentaje + validation_percentaje) * count):\n",
    "                data_validation['in'].append(decoder(filepath))\n",
    "                data_validation['out'].append(index_genre(genre, genres))\n",
    "            else:\n",
    "                data_test['in'].append(decoder(filepath))\n",
    "                data_test['out'].append(index_genre(genre, genres))\n",
    "\n",
    "    data_training = {'in': np.array(data_training['in']),'out': np.array(data_training['out'])}            \n",
    "    data_validation = {'in': np.array(data_validation['in']),'out': np.array(data_validation['out'])}            \n",
    "    data_test = {'in': np.array(data_test['in']),'out': np.array(data_test['out'])}            \n",
    "\n",
    "    return {\n",
    "        'data_training': data_training,\n",
    "        'data_validation': data_validation,\n",
    "        'data_testing': data_test\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get data for MFCC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "data_path = 'url'\n",
    "\n",
    "def read_image(filepath):\n",
    "    global size_images\n",
    "    image = cv2.imread(filepath)\n",
    "    return cv2.resize(image, size_images)\n",
    "\n",
    "def get_data_mfcc(training_percentaje=0.8, validation_percentaje=0.1, test_percentaje=0.1):\n",
    "    return get_data(data_path, genres, read_image,training_percentaje,validation_percentaje,test_percentaje)\n",
    "\n",
    "mfcc_data = get_data_mfcc(train_percentaje, validation_percentaje, test_percentaje)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get data for wavelet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from collections import Counter\n",
    "import scipy\n",
    "import dtcwt\n",
    "\n",
    "data_path = 'url'\n",
    "\n",
    "trans = dtcwt.Transform1d(biort='antonini', qshift='qshift_d')\n",
    "\n",
    "def calculate_entropy(list_values):\n",
    "\tcounter_values = Counter(list_values).most_common()\n",
    "\tprobabilities = [elem[1]/len(list_values) for elem in counter_values]\n",
    "\tentropy=scipy.stats.entropy(probabilities)\n",
    "\treturn entropy\n",
    "\n",
    "def calculate_statistics(list_values):\n",
    "\tn5 = np.nanpercentile(list_values, 5)\n",
    "\tn25 = np.nanpercentile(list_values, 25)\n",
    "\tn75 = np.nanpercentile(list_values, 75)\n",
    "\tn95 = np.nanpercentile(list_values, 95)\n",
    "\tmedian = np.nanpercentile(list_values, 50)\n",
    "\tmean = np.nanmean(list_values)\n",
    "\tstd = np.nanstd(list_values)\n",
    "\tvar = np.nanvar(list_values)\n",
    "\trms = np.nanmean(np.sqrt(list_values**2))\n",
    "\treturn [n5, n25, n75, n95, median, mean, std, var, rms]\n",
    "\n",
    "def calculate_crossings(list_values):\n",
    "\tzero_crossing_indices = np.nonzero(np.diff(np.array(list_values) > 0))[0]\n",
    "\tno_zero_crossings = len(zero_crossing_indices)\n",
    "\tmean_crossing_indices = np.nonzero(np.diff(np.array(list_values) > np.nanmean(list_values)))[0]\n",
    "\tno_mean_crossings = len(mean_crossing_indices)\n",
    "\treturn [no_zero_crossings, no_mean_crossings]\n",
    "\n",
    "def get_features(list_values):\n",
    "\tentropy = calculate_entropy(list_values)\n",
    "\tcrossings = calculate_crossings(list_values)\n",
    "\tstatistics = calculate_statistics(list_values)\n",
    "\treturn [entropy] + crossings + statistics\n",
    "\n",
    "def extract_dtcwt(file_path:str):    \n",
    "    d, fs = librosa.load(file_path)\n",
    "    forw = trans.forward(d, nlevels=17)\n",
    "    features = []\n",
    "    for coeff in forw.highpasses:\n",
    "        temp = (np.abs(coeff.squeeze()))\n",
    "        features += get_features(temp)\n",
    "        \n",
    "    features += get_features(forw.lowpass.squeeze())    \n",
    "    return features\n",
    "\n",
    "def get_data_wavelet(training_percentaje=0.8, validation_percentaje=0.1, test_percentaje=0.1):\n",
    "    return get_data(data_path, genres, extract_dtcwt, training_percentaje,validation_percentaje,test_percentaje)\n",
    "\n",
    "wavalet_data = get_data_mfcc(train_percentaje, validation_percentaje, test_percentaje)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Conv1D data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/content/drive/MyDrive/GTZAN/Ensemble_Data/audios/models/'\n",
    "\n",
    "# CARGAR UN ARCHIVO DE AUDIO\n",
    "def load_audio_file(file_path):\n",
    "    input_length = 660000  # This is 15 seconds with 44100 sample rate\n",
    "    data = librosa.core.load(file_path, sr=22050)[0]  # We use librosa to load audio file with sample rate 22050\n",
    "    if len(data) > input_length:\n",
    "        data = data[:input_length]\n",
    "    else:\n",
    "        data = np.pad(data, (0, max(0, input_length - len(data))), \"constant\")\n",
    "    return data\n",
    "\n",
    "def get_data_conv1D(training_percentaje=0.8, validation_percentaje=0.1, test_percentaje=0.1):\n",
    "    return get_data(data_path, genres, load_audio_file,training_percentaje,validation_percentaje,test_percentaje)\n",
    "\n",
    "conv1D_data = get_data_conv1D()\n",
    "\n",
    "conv1D_data['data_training']['in'] = conv1D_data['data_training']['in'] \n",
    "conv1D_data['data_validation']['in'] = conv1D_data['data_validation']['in']\n",
    "conv1D_data['data_testing']['in'] = conv1D_data['data_testing']['in']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill all datas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = { \n",
    "    'mfcc': mfcc_data,\n",
    "    'wavalet': wavalet_data\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import model.h5 for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'mfcc': None,\n",
    "    'wavalet': None\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Test (Ensemble Model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "classes = len(genres)\n",
    "\n",
    "def train():\n",
    "    train_data = []\n",
    "    models_predict = []\n",
    "    models_predict_validation = []\n",
    "\n",
    "    for key,model_ in models.items():\n",
    "        models_predict.append(model_.predict(data[key]['data_training']['in']))\n",
    "        models_predict_validation.append(model_.predict(data[key]['data_validation']['in']))\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=len(models_predict), activation='relu'))\n",
    "    model.add(Dense(10, activation='sigmoid'))\n",
    "    model.add(Dense(classes, activation='softmax'))\n",
    "\n",
    "    model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "    #Cambiar filas por columnas para tener n datos de tamanno len(modelos), que cada uno seria un array con la salida de cada modelo evaluado en los datos\n",
    "\n",
    "    return model\n",
    "\n",
    "model = train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    models_predict_test = []\n",
    "    for key,model_ in models.items():\n",
    "        models_predict_test.append(model_.predict(data[key]['data_testing']['in']))\n",
    "   \n",
    "    input = None # aqui hay que poner la representacion en columnas de esa talla\n",
    "    output = mfcc_data['data_testing']['out']\n",
    "\n",
    "    score = model.evaluate(input, output, verbose=0)\n",
    "    print('testing accuracy: ' + str(score[1]))\n",
    "\n",
    "test()    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
